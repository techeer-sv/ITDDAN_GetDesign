# Web Crawler

### 2. 핵심 엔티티

-   텍스트 데이터: 웹크롤러의 최종 출력물, 저장될 데이터
-   url 메타 데이터: url 자체 및 해당 url이 크롤링 되었는지 여부
-   도메인 메타데이터: robots.txt 정책 준수를 위해 도메인 수준의 정보 저장

### 3. 인터페이스

-   입력: 크롤링을 시작할 시드 url
-   출력: 웹사이트에서 파싱된 텍스트 데이터

### 4. 데이터 플로우

1. 프론티어 세트에서 시드 Url 가져옴 (아직 크롤링 되지 않는 Url 집합)
2. DNS에서 해당 URL의 IP 요청
3. HTML 페이지 가져옴
4. 페이지에서 텍스트 추출해서 저장함
5. 텍스트 내의 다른 URL 추출해서 프론티어 세트에 추가함
6. 프론티어 세트가 빌 때까지 반복함

---

# 처음 흐름

## 1. Frontier Queue

## 2. Web Crawler

-   큐에서 url 꺼내기
-   웹페이지 가져오기
-   텍스트 추출
-   url 추출

## 3. DB

-   Blog Storage : S3

---

# Deep Dive

## 1. 크롤러가 많은 일을 하고 있다. : Fault Tolerance

-   디비에 저장중간 단계에서 실패가 되면 큐에 여전히 존재한다

### ⇒ 크롤러를 더 작은 합리적인 단계로 나누기 (파이프라인 분할)

1. HTML을 가져오기
    - 가장 오류가 발생하기 쉬운 작업이니 격리
2. HTML 파싱

    ### ⇒ Parsing Queue 추가

    - 구분 분석 큐 추가
    - URL : id/url/s3linktoHTML
    - 1단계에서 HTML이 s3에 저장되면 해당 링크를 파싱 큐에 넣는다
    - 파싱 큐에서 S3링크를 가져와서 HTML을 읽고, 텍스트를 추출하며, 새로운 url을 추출해 프론티어 큐로 다시 넣는다
    - 추출된 최종 텍스트 데이터는 S3에 저장된다.3

## 2. 재시도 전략 Retry Strategy

-   웹사이트 접속 실패(500에러, 서버 다운)는 흔함
-   인메모리 타이머 재시도는 크롤러가 다운됐을 때 진행상황을 잃음, 불특정한 재시도 간격 문제를 가짐

### ⇒ SQS를 사용해서 구성 가능한 지수 백오프를 통한 재시도 구현

-   SQS는 기본적으로 메시지 처리 실패 시 가시성 타임아웃 기간 동안 재시도 하며 이 타임아웃은 지수적으로 증가한다.
-   최대 재시도 횟수를 설정하고 초과하면 메시지를 데드 레터 큐(DLQ)로 자동 이동 시켜 무한 재시도를 방지하고 실패한 URL 관리가 가능함

### 크롤러 작업자가 URL 처리 중 다운되면?

-   SQS의 가시성 타임아웃으로 URL은 큐에 남아있다가 타임아웃이 만료되면 다른 크롤러가 가져감

## 3. Politeness 규칙 준수

-   웹사이트 서버에 과부하 주지 않도록 robots.txt 파일에 명시된 규칙 준수해서 과도한 요청을 보내지 말아야함

### ⇒ 도메인 테이블 사용

-   도메인, 마지막 크롤링 시간, robots.txt에서 파싱한 규칙(사용자 에이전트, 금지 경로, 크롤링 지연 시간 저장)
-   크롤러는 URL을 가져올 때마다 해당 도메인의 robots.txt 확인
    -   url이 금지된 경우 해당 메시지를 큐에서 제거
-   마지막 크롤링 시간과 로봇.ㅌ의 지연 시간을 비교해서 허용되지 않았으면 SQS가 가시성 타임아웃을 크롤링 지연 시간으로 설정하여 메시지를 큐에 다시 넣어둠

### ⇒ Redis 인메모리로 도메인별 속도 제한 장치 구현

-   일반적인 속도 제한(크롤링 지연이 명시되지 않더라도)은 초당 1회 이상은 지양해야함
-   크롤러가 웹페이지 가져오기 전에 속도 제한 장치 확인
-   요청이 허용되지 않으면 지터와 함께 큐에 다시 경로를 넣어둠
    -   여러 크롤러가 동시에 같은 도메인에 대한 요청을 재시도해서 병목 현상 일으키는 것 방지

<aside>
👉

선택 심화: 스마트 스케줄러 (토론 주제)

-   동일 도메인에서 파생된 수많은 url이 큐에 쌓여서 크롤러가 동일 도메인에 반복적으로 접근하여 속도 제한에 걸리는 비효율성이 발생할 수 있음
-   url 메타데이터에서 url을 가져올 때 알고리즘 기반으로 크롤링 순서를 결정하거나 우선순위를 할당하여 큐에 넣음
</aside>

## 4. 확장성 및 효율성

-   5일 이내 100억개의 웹 페이지를 크롤링하고, 이미 크롤링 했거나 중복된 콘텐츠를 다시 처리하는 비효율성을 피해야함

### ⇒ 필요 크롤러 수 추정

-   평균 페이지 크기(2mb)와 네트워크 대역폭(400gbps)를 기반으로, 실제 사용 가능한 대역폭(예: 30퍼)을 고려하여 필요한 크롤러 인스턴스 수를 대략적으로 추정함
    -   결과적으로는 100억페이지 5일 내 처리하려면 슈퍼 듀티 네트워크 최적화 aws 인스턴스 4개 정도가 필요함

### ⇒ DNS 병목 현상

-   타사 DNS 제공업체는 속도 제한이 있어 대규모 크롤링 시 병목 현상이 될 수 있음
-   DNS 캐싱: redis로 DNS 쿼리 캐싱해서 요청 수 크게 줄임
-   다중 DNS 제공 업체 사용하기

### ⇒ 중복 콘텐츠 처리 효율성

-   이미 크롤링 된 URL을 다시 크롤링 하거나 내용이 중복되는 웹 페이지를 다시 파싱하는 것은 비효율적

1. 디비에서 url 메타데이터의 url을 기본키로 설정해서 중복 추가 방지
    - 100억 개 이상의 url 저장하려면 디비는 샤딩 되어야함
2. 크롤러가 HTML을 가져오면 HTML을 해시
    - 해시 값 저장하고 전역 보조 인덱스를 생성해서 해시가 이미 존재하는지 효율적으로 확인하도록
        - 대안으로 레디스 인메모리 해시 세트를 사용해서 해시 저장하고 포함 여부를 빠르게 확인할 수 있음
3. 크롤러 트랩 회피
    - 크롤러를 무한정 사이트에 묶어두도록 설계된 페이지(동일 페이지가 무수히 많은 링크로 연결되거나 내용이 거의 없는 페이지 무한 반복)
    - url 메타 데이터에 깊이(depth) 필드 추가
        - 새로운 url 추가할 때 현재 파싱 중인 URL의 깊이에 1을 더해서 새 깊이 설정
        - 미리 정의된 최대 깊이 임계값(20)을 초과하면 프론티어 큐에 더 추가되지 않음
