# 영상 보기 전의 내 설계


## 1. 각 단계 설명

### 1.1 Client → API Gateway
- 초기 seed UR 입력

### 1.2 URL 수신기
- `robots.txt` 확인
- 허용된 URL만 Queue에 전달

### 1.3 Queue 적재
- 병렬 처리를 위한 작업 분배
- 실패 재시도, 우선순위 제어 가능

### 1.4 HTML Fetch
- HTTP 요청으로 웹페이지 HTML 다운로드

### 1.5 HTML Parsing
- HTML 문서에서 텍스트, 링크, 메타데이터 등 추출

### 1.6 MongoDB 저장
- 파싱된 데이터를 비정형 문서 형태로 저장


## 2. 큐 시스템 선택지

| 큐 종류        | 장점                            | 단점                                | 비고                         |
|----------------|----------------------------------|--------------------------------------|------------------------------|
| Redis (RQ 등)  | 빠름, 간단                       | 기능 제한, 확장성 낮음              | 소규모 프로젝트에 적합        |
| RabbitMQ       | 메시지 순서 보장, 안정적         | 설정 복잡, 확장 한계                | 안정성과 순서가 중요할 때     |
| Apache Kafka   | 고속 처리, 대용량 스트리밍에 적합 | 복잡도 높음, 학습 곡선 있음         | 대규모 분산 크롤러 시스템에 적합 |

>  RabbitMQ (기본 안정성 위주) 또는 Kafka (대규모/확장성 우선)


## 3. 데이터 저장소: MongoDB 및 대안

### 3.1 MongoDB

- **장점**:  
  - JSON 기반의 비정형 데이터에 적합  
  - Sharding 지원 → 대규모 분산 저장 가능

- **단점**:  
  - Full-text 검색에는 다소 부족  
  - 초대규모 저장 시 성능 튜닝 필요


# 영상과의 차이 

## 1. 데이터 저장소 구조
### 내 설계
- MongoDB를 단일 저장소로 사용

### 개선점
- **다중 저장소 전략**
  - 원본 HTML 데이터 → S3 (Blob Storage)
  - 추출된 텍스트 데이터 → S3
  - URL 메타데이터 → DynamoDB/PostgreSQL
    - 크롤링 상태
    - S3 링크
    - 마지막 크롤링 시간
  - 도메인 메타데이터 → 별도 테이블
    - robots.txt 정보
    - 크롤링 정책

## 2. 결함 허용 및 견고성
### 내 설계
- 기본적인 큐 기반 비동기 처리만 고려

### 개선점
- **정교한 재시도 전략**
  - 지수 백오프(exponential backoff) 적용
  - Amazon SQS의 가시성 타임아웃 활용
  - 재시도 횟수 제한 (예: 5회)
- **Dead Letter Queue (DLQ)**
  - 지속적 실패 URL 격리
  - 문제 URL 모니터링 및 분석 가능

## 3. 정중함(Politeness) 구현
### 내 설계
- robots.txt 확인 정도만 명시

### 개선점
- **체계적인 정책 관리**
  - robots.txt 파싱 결과 DB 저장
    - disallow 경로
    - crawl-delay 시간
  - Redis 기반 레이트 리미터
    - 도메인당 초당 요청 제한
    - Jitter 추가로 동시 재시도 방지
- **스마트 스케줄러**
  - URL 우선순위 기반 큐잉
  - 동일 도메인 URL 분산 처리

## 4. 확장성 및 효율성
### 내 설계
- 목표 명시 (100억 페이지/5일)
- 구체적 구현 방안 부족

### 개선점
- **리소스 최적화**
  - BOE 계산 기반 크롤러 머신 수 산정
  - 파싱 워커의 동적 스케일링
- **DNS 최적화**
  - Redis 기반 DNS 캐싱
  - 다중 DNS 제공업체 활용
- **중복 처리 방지**
  - URL 메타데이터로 중복 URL 필터링
  - HTML 해싱으로 중복 콘텐츠 감지
    - URL 메타데이터 GSI 활용
    - Redis 인메모리 해시 세트 활용
- **크롤러 트랩 방지**
  - 최대 크롤링 깊이 설정
  - 무한 루프 및 저가치 페이지 방지
